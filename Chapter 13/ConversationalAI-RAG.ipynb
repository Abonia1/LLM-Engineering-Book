{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd20d6a",
   "metadata": {},
   "source": [
    "\n",
    "## Building a Conversational Chatbot with Custom Data \n",
    "\n",
    "This notebook guides you through the creation of a chatbot tailored to your specific data needs. Utilizing HuggingFaceEmbeddings and FAISS, the project transforms documents into vectors for a local vector storage system. Then it integrates the \"meta-llama/llama-2-7b-chat\" model from your local machine. The `langchain` library plays a crucial role in this process, aiding in tasks like chunking documents, indexing data in vector db, managing conversation chains with memory buffers, and crafting prompt templates.\n",
    "\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **PDF Content Processing**: When users upload PDF files, the notebook extracts the text, segments it into manageable chunks, and indexes these chunks in in a vector db locally using HuggingFaceEmbeddings and FAISS.\n",
    "- **Data-Driven Query Handling**: Users can pose questions to the chatbot, which searches the indexed data for relevant answers.\n",
    "- **Integrating Vector Database and LLMs**: We leverage `langchain`'s capabilities to link vector database indexing with llama-2 LLMs, enabling a seamless conversational experience with memory and retrieval functionalities.\n",
    "- **Hallucination Check**: The notebook includes a mechanism to detect and correct any hallucinations or inaccuracies in the LLM's responses.\n",
    "\n",
    "### Prerequisites for Running the Notebook:\n",
    "\n",
    "\n",
    "1. **Library Requirements**: Confirm that you have installed all libraries specified in the `requirements.txt` file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fad898",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf langchain langchain-community tiktoken llama-cpp-python panel streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2370713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: htmltemplate in /Users/aboniasojasingarayar/miniconda3/lib/python3.11/site-packages (2.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install htmltemplate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e379a0a",
   "metadata": {},
   "source": [
    "Below cell imports the required libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c4177b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aboniasojasingarayar/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import PyPDF2\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings # import hf embedding\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c264d9e",
   "metadata": {},
   "source": [
    "### Enter your pdf file name below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57efcd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_docs=[\"./data/CV.pdf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7158eb08",
   "metadata": {},
   "source": [
    "### Step 1: Prepare above documents and their metadata\n",
    "The prepare_docs function below processes a list of PDF documents by extracting text from each page and organizing it into two lists: one for the text content and another for the metadata (titles). It iterates through each page of each PDF, extracts the text, and forms a title using the PDF name and page number. The function returns these two lists, making it useful for indexing and referencing the content of multiple PDFs at a page level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97e49bddd35c6d1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T08:05:12.765779Z",
     "start_time": "2024-01-07T08:05:12.763477Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def prepare_docs(pdf_docs):\n",
    "    docs = []\n",
    "    metadata = []\n",
    "    content = []\n",
    "\n",
    "    for pdf in pdf_docs:\n",
    "\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf)\n",
    "        for index, text in enumerate(pdf_reader.pages):\n",
    "            doc_page = {'title': pdf + \" page \" + str(index + 1),\n",
    "                        'content': pdf_reader.pages[index].extract_text()}\n",
    "            docs.append(doc_page)\n",
    "    for doc in docs:\n",
    "        content.append(doc[\"content\"])\n",
    "        metadata.append({\n",
    "            \"title\": doc[\"title\"]\n",
    "        })\n",
    "    print(\"Content and metadata are extracted from the documents\")\n",
    "    return content, metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921288be",
   "metadata": {},
   "source": [
    "### Step 2: Chunk the documents \n",
    "The get_text_chunks function takes text content and metadata as inputs and splits the content into smaller chunks. It uses a RecursiveCharacterTextSplitter configured with a specified chunk size (512 characters) and overlap (256 characters) for this purpose. The function processes the content, splitting it into passages while maintaining associated metadata. After splitting, it prints the total number of passages created and returns these split documents. This function is useful for breaking down large text into more manageable, indexed segments for easier processing and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35e8895acb436e94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T07:47:25.155847Z",
     "start_time": "2024-01-07T07:47:25.149629Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_text_chunks(content, metadata):\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=256,\n",
    "    )\n",
    "    split_docs = text_splitter.create_documents(content, metadatas=metadata)\n",
    "    print(f\"Documents are split into {len(split_docs)} passages\")\n",
    "    return split_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f576d0",
   "metadata": {},
   "source": [
    "### Step 3: Ingest into Vector Database locally\n",
    "\n",
    "The `ingest_into_vectordb` function is designed for processing and indexing a collection of documents into a vector database using FAISS (Facebook AI Similarity Search) for efficient similarity searches. It operates as follows:\n",
    "\n",
    "1. **Embedding Creation**: It generates embeddings for the input documents (`split_docs`) using the Hugging Face model `'sentence-transformers/all-MiniLM-L6-v2'`. This model is specifically chosen for its efficiency in creating sentence-level embeddings and is set to run on the CPU.\n",
    "\n",
    "2. **Vector Database Indexing**: Utilizes the generated embeddings to create a FAISS vector database. FAISS is used for its ability to efficiently handle large-scale similarity searches and clustering of dense vectors.\n",
    "\n",
    "3. **Local Storage**: After creating the vector database, the function saves it locally to the path specified by `DB_FAISS_PATH`, ensuring the data can be easily accessed for future similarity searches or retrieval tasks.\n",
    "\n",
    "The primary purpose of this function is to transform textual data into a structured, searchable vector format, facilitating efficient and scalable retrieval tasks such as document similarity searches or clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba2d9675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_into_vectordb(split_docs):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cpu'})\n",
    "    db = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "    DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "    db.save_local(DB_FAISS_PATH)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ba4cd",
   "metadata": {},
   "source": [
    "### Step 4: Set up Conversation Chain using LLM\n",
    "The `get_conversation_chain` function is designed to create and configure a conversational chain for a language model, specifically using the LLaMA model and a vector database for retrievals. Here's a summary of its main components and functionalities:\n",
    "\n",
    "1. **Callback Manager Setup**:\n",
    "   - Initializes a `CallbackManager` with `StreamingStdOutCallbackHandler()`, which likely handles streaming and logging outputs during the model's operation.\n",
    "\n",
    "2. **LLaMA Model Configuration**:\n",
    "   - Instantiates a `LlamaCpp` model with specified parameters such as `model_path`, `temperature`, `max_tokens`, `top_p`, and `n_ctx`. These parameters configure the behavior of the LLaMA model, including its conversational style and technical constraints.\n",
    "   - Integrates the `callback_manager` with the LLaMA model, allowing for additional processing or logging during the model's operation.\n",
    "\n",
    "3. **Retriever Initialization**:\n",
    "   - Transforms the input `vectordb` into a retriever, enabling it to fetch relevant information from the vector database during conversations.\n",
    "\n",
    "4. **Conversation Chain Creation**:\n",
    "   - Sets up a `ConversationBufferMemory`, which manages the conversation history and assists in generating context-aware responses.\n",
    "   - Constructs a `ConversationalRetrievalChain` using the LLaMA model (`llama_llm`), the retriever, and the conversation memory. This chain is responsible for handling the flow of the conversation, including retrieving relevant information and generating responses.\n",
    "\n",
    "5. **Return Value**:\n",
    "   - Outputs a message indicating the successful creation of the conversational chain.\n",
    "   - Returns the `conversation_chain` object, which can be used to handle conversational interactions using the LLaMA model and the vector database.\n",
    "\n",
    "This function sets up a sophisticated conversational AI system combining the LLaMA model for language generation and a vector database for information retrieval, enhanced with a callback manager for additional processing and a conversation memory buffer for context management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daeb1adc421d294e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T07:48:11.383224Z",
     "start_time": "2024-01-07T07:48:11.380239Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "template = \"\"\"[INST]\n",
    "As an AI, provide accurate and relevant information based on the provided document. Your responses should adhere to the following guidelines:\n",
    "- Answer the question based on the provided documents.\n",
    "- Be direct and factual, limited to 50 words and 2-3 sentences. Begin your response without using introductory phrases like yes, no etc.\n",
    "- Maintain an ethical and unbiased tone, avoiding harmful or offensive content.\n",
    "- If the document does not contain relevant information, state \"I cannot provide an answer based on the provided document.\"\n",
    "- Avoid using confirmatory phrases like \"Yes, you are correct\" or any similar validation in your responses.\n",
    "- Do not fabricate information or include questions in your responses.\n",
    "- do not prompt to select answers. do not ask me questions\n",
    "{question}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "#template = \"\"\"Given the document and the current conversation between a user and an agent, your task is as follows: Answer any user query by using information from the document. The response should be detailed.\"\"\"\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "def get_conversation_chain(vectordb):\n",
    "    llm = Ollama(model=\"mistral\")\n",
    "\n",
    "    retriever = vectordb.as_retriever()\n",
    "    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history', return_messages=True, output_key='answer')\n",
    "\n",
    "    conversation_chain = (ConversationalRetrievalChain.from_llm\n",
    "                          (llm=llm,\n",
    "                           retriever=retriever,\n",
    "                           memory=memory,\n",
    "                           return_source_documents=True))\n",
    "    print(\"Conversational Chain created for the LLM using the vector store\")\n",
    "    return conversation_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e69dfd",
   "metadata": {},
   "source": [
    "### Step 5: Detect Hallucination in the LLMs Response\n",
    "The `validate_answer_against_sources` function evaluates the reliability of a response by comparing it with source documents. It works as follows:\n",
    "\n",
    "1. **Model Initialization**: Utilizes the SentenceTransformer model 'all-MiniLM-L6-v2' to generate embeddings.\n",
    "\n",
    "2. **Threshold Setting**: Sets a similarity threshold (here, 0.5) to determine the acceptable level of similarity between the response and source documents.\n",
    "\n",
    "3. **Extracting Source Texts**: Gathers the content of the source documents.\n",
    "\n",
    "4. **Computing Embeddings**: Generates embeddings for both the response answer and the source texts.\n",
    "\n",
    "5. **Calculating Similarity**: Computes cosine similarity scores between the response answer's embedding and the embeddings of each source text.\n",
    "\n",
    "6. **Validity Check**: Checks if any of the similarity scores exceed the set threshold. If yes, it implies that the response is sufficiently similar to at least one of the source documents, suggesting its reliability, and returns `True`. If not, it returns `False`.\n",
    "\n",
    "Essentially, this function serves as a mechanism to check the alignment of the chatbot's response with the information in the source documents, ensuring the response's accuracy and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d0cc0f87a9595c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T07:48:35.893137Z",
     "start_time": "2024-01-07T07:48:35.887077Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def validate_answer_against_sources(response_answer, source_documents):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    similarity_threshold = 0.5  \n",
    "    source_texts = [doc.page_content for doc in source_documents]\n",
    "\n",
    "    answer_embedding = model.encode(response_answer, convert_to_tensor=True)\n",
    "    source_embeddings = model.encode(source_texts, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.pytorch_cos_sim(answer_embedding, source_embeddings)\n",
    "\n",
    "\n",
    "    if any(score.item() > similarity_threshold for score in cosine_scores[0]):\n",
    "        return True  \n",
    "\n",
    "    return False  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f38df5c",
   "metadata": {},
   "source": [
    "Now that we have crafted all the necessary functions, it's time to put them into action and test their functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e435e003cfe91c1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:24:11.041141Z",
     "start_time": "2024-01-07T10:24:10.938343Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content and metadata are extracted from the documents\n"
     ]
    }
   ],
   "source": [
    "content, metadata = prepare_docs(pdf_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62e99300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents are split into 19 passages\n"
     ]
    }
   ],
   "source": [
    "split_docs = get_text_chunks(content, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a43b4af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb=ingest_into_vectordb(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79d503befc592a5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:44:24.130465Z",
     "start_time": "2024-01-07T10:44:22.503570Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversational Chain created for the LLM using the vector store\n"
     ]
    }
   ],
   "source": [
    "conversation_chain=get_conversation_chain(vectordb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df276c10",
   "metadata": {},
   "source": [
    "### Ask your Question\n",
    "\n",
    "We created a conversational chain and now ready to chat with your own data. \n",
    "\n",
    "\n",
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0c000474595b40e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:44:54.014449Z",
     "start_time": "2024-01-07T10:44:50.322823Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aboniasojasingarayar/miniconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  who is Abonia Sojasingarayar?\n",
      "A:   Abonia Sojasingarayar is a Machine Learning Scientist, Data Scientist, NLP Engineer, Computer Vision Engineer, AI Analyst, and Technical Writer. They have education from the Université Pondicherry in India, IA School in Boulogne-Billancourt, France, and Institut F2I in Paris, France. Abonia has certifications from IBM and deeplearning.IA, and they are proficient in various tools and techniques related to their field such as Python, TensorFlow, GCP professional data engineer Badges, Watson Assistant, and RPA (Robotic Process Automation) among others. They have worked on projects involving API integration, machine learning pipeline development, and research engineering.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"who is Abonia Sojasingarayar?\"\n",
    "response=conversation_chain({\"question\": user_question})\n",
    "print(\"Q: \",user_question)\n",
    "print(\"A: \",response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b0d849",
   "metadata": {},
   "source": [
    "We have now received an answer for a provided question. We can also view the conversation history and source documents in the response.\n",
    "\n",
    "\n",
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "269d50b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  where did she graduated?\n",
      "A:   Abonia Sojasingarayar graduated from the Université Pondicherry in India with a licence en technologie informatique et Ingénierie degree.\n",
      "\n",
      "Conversation Chain: \n",
      " {'question': 'where did she graduated?', 'chat_history': [HumanMessage(content='who is Abonia Sojasingarayar?'), AIMessage(content=' Abonia Sojasingarayar is a Machine Learning Scientist, Data Scientist, NLP Engineer, Computer Vision Engineer, AI Analyst, and Technical Writer. They have education from the Université Pondicherry in India, IA School in Boulogne-Billancourt, France, and Institut F2I in Paris, France. Abonia has certifications from IBM and deeplearning.IA, and they are proficient in various tools and techniques related to their field such as Python, TensorFlow, GCP professional data engineer Badges, Watson Assistant, and RPA (Robotic Process Automation) among others. They have worked on projects involving API integration, machine learning pipeline development, and research engineering.'), HumanMessage(content='where did she graduated?'), AIMessage(content=' Abonia Sojasingarayar graduated from the Université Pondicherry in India with a licence en technologie informatique et Ingénierie degree.')], 'answer': ' Abonia Sojasingarayar graduated from the Université Pondicherry in India with a licence en technologie informatique et Ingénierie degree.', 'source_documents': [Document(page_content='Abonia Sojasingarayar  \\n  \\n \\n \\nMachine Learning Scientist | Data Scientist | NLP Engineer | Computer Vision Engineer | AI \\nAnalyst | Technical Writer  \\n \\nAbonia  SOJASINGARAYAR                                                                       01/2023 Abonia SOJASINGARAYAR  \\nData scientist - Machine Learning \\nEngineer  – Natural Language \\nProcessing et Computer Vision \\nEngineer   \\n \\n \\nFormation et Qualification  \\n &Education  \\n   \\n \\nFormation  \\n \\n \\n \\n \\nCertification et \\nBadge  \\n \\n \\n \\n \\n \\n \\n \\n \\nLangues  \\n \\n \\n \\n \\nMedium Writer  \\n \\n \\n \\nLinkedin & \\nPublications   \\n2021  : M2 - Intelligence  Artificielle  \\nIA School – Boulogne -Billancourt  \\n2020  : M1 - Chef De Projet Digital   \\nInstitut F2I  – Paris (France)  \\n2015  : Licence en technologie informatique et Ingénierie  \\nUniversité Pondicherry  – Karikal (Inde)  \\n \\n \\nGoogle certified professional data engineer  \\nIBM Quantum Conversation Badge  \\nMachine learning specialization par deeplearning.IA , Andrew NG  \\nDeep learning & natural language specialization par deeplearning.IA,Andrew NG  \\nSpark par learning Academy  \\nAdvance Python bootcamp  \\nGCP professional data engineer Badges  \\nWatson Assistant Hands on  \\nMéthodologie Watson Assistant  \\nMéthodologie Watson Fondation   \\nWatson Assistant Hands on  \\nMéthodologie  Watson Knowledge Studio   \\nWatson Knowledge Studio  Hands on', metadata={'title': './data/CV.pdf page 1'}), Document(page_content=\"Abonia Sojasingarayar  \\n  \\n \\n \\nMachine Learning Scientist | Data Scientist | NLP Engineer | Computer Vision Engineer | AI \\nAnalyst | Technical Writer  \\n \\nAbonia  SOJASINGARAYAR                                                                       01/2023 • Résolution des problèmes de sécurité et de sauvegarde des données.  \\n• Clonage de disque dur et configuration des paramètres du disque dur  \\nOutils et techniques  : Clone de disque dur, maintien du poste PC, réparer \\nl'imprimante…\", metadata={'title': './data/CV.pdf page 7'}), Document(page_content=\"Abonia Sojasingarayar  \\n  \\n \\n \\nMachine Learning Scientist | Data Scientist | NLP Engineer | Computer Vision Engineer | AI \\nAnalyst | Technical Writer  \\n \\nAbonia  SOJASINGARAYAR                                                                       01/2023  \\n \\n \\n \\n \\n \\n \\nJuin 2021 –  \\nJuillet 2021  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nNov 2021 –  \\nJuin 2021  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nJuillet 2021 –  \\nOct 2021  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nMai 2021 –  \\nJuin 2021  • Assister à une présentation de formation une fois/semaine  \\nOutils et techniques : Watson discovey search , LDA anaysis, Gensim , NLTK,  \\nspaCy … \\n \\nMission Allianz France et Whatfix – API integration Engineer  \\n• Assistant Watson et intégration API pour la plateforme Digital Coach - \\nAllianz France.  \\n• Entrainement  et intégration du chatbot d'assistance watson  \\n• Spécification de  open API :  intégration avec IBM app connect  \\n• Intégration de l'API Watson avec l'équip e Whatfix d’Inde  \\nOutils et techniques : Watson Assistant , whatfix platform, WA API…  \\n \\n \\nMission BNPP  – RPA Developer  \\n• RPA et OCR  \\n• Automatisation intelligente des processus bancaires et d'assurance avec\", metadata={'title': './data/CV.pdf page 4'}), Document(page_content=\"Abonia Sojasingarayar  \\n  \\n \\n \\nMachine Learning Scientist | Data Scientist | NLP Engineer | Computer Vision Engineer | AI \\nAnalyst | Technical Writer  \\n \\nAbonia  SOJASINGARAYAR                                                                       01/2023  \\n \\n \\n \\nExpérience professionnelle  & \\n \\nProfil  : Je suis passionnée par l'intelligence artificielle, le deep learning, l'apprentissage automatique, le \\ntraitement du langage naturel et la vision par ordinateur. J'ai précédemment travaillé dans les domaines \\ndes solutions de gestion RH, l'industrie chimique  et la cyber sécurité . Ma mission personnelle est de créer \\ndes solutions basées sur l'IA qui résolvent des problèmes à fort impact pour les gens du monde entier et \\nsimplifient la vie quotidienne.  \\n \\n \\nCompétences fonctionnelles  \\n• Distribution  (Ventes - E-Commerce etc… ) \\n• Industrie  (Chimique , IT, Solution RH, Cyber sécurité ) \\n \\nCompétences techniques   \\n• Méthodes et Outils →Développent  en mode Agile  \\n• Modélisation : Algorithmes de classifications (SVM, SGD, MLP, Arbre de \\ndécision, Réseaux de  Neurones…), API développement, optimisation de \\nhyperparamètre, régularisation etc  \\n• Recherche et développement d ’un outil d’extraction de contenu.  \\n• Implémentation d ’une bibliothèque de prétraitement de texte.\", metadata={'title': './data/CV.pdf page 2'})]}\n"
     ]
    }
   ],
   "source": [
    "user_question = \"where did she graduated?\"\n",
    "response=conversation_chain({\"question\": user_question})\n",
    "print(\"Q: \",user_question)\n",
    "print(\"A: \",response['answer'])\n",
    "print(\"\\nConversation Chain: \\n\",response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
